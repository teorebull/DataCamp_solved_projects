{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1309988-b429-4fb0-8c4c-193582dbec93",
   "metadata": {},
   "source": [
    "![mobydick](mobydick.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611e416c-70e7-478a-a3c8-e54f3fdb4a7f",
   "metadata": {},
   "source": [
    "In this workspace, you'll scrape the novel Moby Dick from the website [Project Gutenberg](https://www.gutenberg.org/) (which contains a large corpus of books) using the Python `requests` package. You'll extract words from this web data using `BeautifulSoup` before analyzing the distribution of words using the Natural Language ToolKit (`nltk`) and `Counter`.\n",
    "\n",
    "The Data Science pipeline you'll build in this workspace can be used to visualize the word frequency distributions of any novel you can find on Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b5f52f-fd9b-4f0e-9fcc-f7733022c7c0",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 50,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1743524829260,
    "lastExecutedByKernel": "a95b3734-e40a-4198-9f10-b4fd35a53ef9",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9443d4a2-8330-4ff5-8667-a64d10a2ab57",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 318,
    "lastExecutedAt": 1743524829578,
    "lastExecutedByKernel": "a95b3734-e40a-4198-9f10-b4fd35a53ef9",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# extract the HTML and create a BeautifulSoup object using an HTML parser to get the text.\n\nimport requests\n\nurl = 'https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm'\n\nr = requests.get(url, 'html.parser')\nr.encoding = 'utf-8'\n\nsoup = BeautifulSoup(r.text)",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# extract the HTML and create a BeautifulSoup object using an HTML parser to get the text.\n",
    "\n",
    "url = 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'\n",
    "\n",
    "response = requests.get(url, 'html.parser')\n",
    "response.encoding = 'utf-8'\n",
    "\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b87ec27e-b2f1-4ed7-8151-8c401031fb5d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 105,
    "lastExecutedAt": 1743524829684,
    "lastExecutedByKernel": "a95b3734-e40a-4198-9f10-b4fd35a53ef9",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# initialize a regex tokenizer object tokenizer to keep only alphanumeric text\n\nimport nltk\n\ntokenizer = nltk.tokenize.RegexpTokenizer(pattern=r'[a-zA-Z0-9]+')\n\ntokens = tokenizer.tokenize(soup.text)",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# initialize a regex tokenizer object tokenizer to keep only alphanumeric text\n",
    "\n",
    "import nltk\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(pattern=r'[a-zA-Z0-9]+')\n",
    "\n",
    "tokens = tokenizer.tokenize(soup.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ea7357-4605-4b4a-88b0-6e01094b27aa",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 56,
    "lastExecutedAt": 1743524829740,
    "lastExecutedByKernel": "a95b3734-e40a-4198-9f10-b4fd35a53ef9",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# transform the tokens into lowercase, removing English stop words\nfrom nltk.corpus import stopwords\n\nstops = set(stopwords.words('english'))\n\ndef words_no_stop(tokens):\n    new_tokens = []\n    for token in tokens:\n        token = token.lower()\n        if token not in stops:\n            new_tokens.append(token)\n    return new_tokens\n\ntokens = words_no_stop(tokens)",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# transform the tokens into lowercase, removing English stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def words_no_stop(tokens):\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if token not in stops:\n",
    "            new_tokens.append(token)\n",
    "    return new_tokens\n",
    "\n",
    "tokens = words_no_stop(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8bf12c7-98f7-4e40-8fe0-91760170cf71",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 60,
    "lastExecutedAt": 1743524829800,
    "lastExecutedByKernel": "a95b3734-e40a-4198-9f10-b4fd35a53ef9",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# initialize a Counter object and find the ten most common words\n\ncounter = Counter(tokens)\n\ndir(counter)\n\ntop_ten = counter.most_common(10)\n\nprint(top_ten)",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('whale', 1240), ('one', 923), ('like', 647), ('upon', 566), ('man', 530), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 450)]\n"
     ]
    }
   ],
   "source": [
    "# initialize a Counter object and find the ten most common words\n",
    "\n",
    "counter = Counter(tokens)\n",
    "\n",
    "dir(counter)\n",
    "\n",
    "top_ten = counter.most_common(10)\n",
    "\n",
    "print(top_ten)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
